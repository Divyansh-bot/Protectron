{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R56GW3S3g6LC",
        "outputId": "d97f0eb0-0cbb-4551-9b28-c7e037b86591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opow6Hnohiyg",
        "outputId": "85c507fe-f36c-4f2e-a9e0-a521d3150d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bx2PyNP9hmLD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "spam_emails_path = os.path.join(\"spamassassin-public-corpus\", \"spam\") #spamassassin-public-corpus/spam\n",
        "ham_emails_path = os.path.join(\"spamassassin-public-corpus\", \"ham\")#spamassassin-public-corpus/ham\n",
        "labeled_file_directories = [(spam_emails_path, 0), (ham_emails_path, 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS6VYtsRjoon"
      },
      "source": [
        "1. `import os`: This statement is used to import the `os` module. This module provides a way of using operating system dependent functionality like reading or writing to the environment, or creating or deleting directories.\n",
        "\n",
        "2. `spam_emails_path = os.path.join(\"spamassassin-public-corpus\", \"spam\")`: Here, the `os.path.join` method is used to join one or more path components intelligently. This method concatenates various path components with exactly one directory separator (`'/'`) following each non-empty part except the last path component. If the last path component to be joined is empty then a directory separator (`'/'`) is put at the end. This will refer to the spam email files located in \"spamassassin-public-corpus/spam\" directory.\n",
        "\n",
        "3. `ham_emails_path = os.path.join(\"spamassassin-public-corpus\", \"ham\")`: This is similar to the previous point but it's setting the path for ham (non-spam) email files located in \"spamassassin-public-corpus/ham\" directory.\n",
        "\n",
        "4. `labeled_file_directories = [(spam_emails_path, 0), (ham_emails_path, 1)]`: This is a list of tuples where each tuple contains a string defined earlier (either `spam_emails_path` or `ham_emails_path`) and an integer which could be a label for spam (0) and ham (1) emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7iZD308qj4O6"
      },
      "outputs": [],
      "source": [
        "email_corpus = []\n",
        "labels = []\n",
        "\n",
        "for class_files, label in labeled_file_directories:\n",
        "    files = os.listdir(class_files)\n",
        "    for file in files:\n",
        "        file_path = os.path.join(class_files, file)\n",
        "        try:\n",
        "            with open(file_path, \"r\") as currentFile:\n",
        "                email_content = currentFile.read().replace(\"\\n\", \"\")\n",
        "                email_content = str(email_content)\n",
        "                email_corpus.append(email_content)\n",
        "                labels.append(label)\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWU3PzNkljxH"
      },
      "source": [
        "1. `email_corpus = []` and `labels = []` -- These lines are initializing two empty lists, `email_corpus` and `labels`.\n",
        "\n",
        "2. `for class_files, label in labeled_file_directories:` -- It starts a for loop that iterates over the `labeled_file_directories`. This is assumed to be a list of tuples where each tuple contains two items, the directory name (`class_files`) and the corresponding label (`label`).\n",
        "\n",
        "3. `files = os.listdir(class_files)` -- It is reads the names of all files in the current directory (`class_files`).\n",
        "\n",
        "4. `for file in files:` -- It starts another for loop that iterates over each file in the directory.\n",
        "\n",
        "5. `file_path = os.path.join(class_files, file)` -- It builds each file's full path by joining the directory name and the file name.\n",
        "\n",
        "6. `try:` -- It starts a try-except block to handle potential errors when opening files. Any errors will be silently ignored due to the subsequent `pass`.\n",
        "\n",
        "7. `with open(file_path, \"r\") as currentFile:` -- It opens the current file.\n",
        "\n",
        "8. `email_content = currentFile.read().replace(\"\\n\", \"\")` -- It reads the entire file content as a string and replaces all newline characters (`\\n`) with an empty string.\n",
        "\n",
        "9. `email_content = str(email_content)` -- It ensures the data read is in string format.\n",
        "\n",
        "10. `email_corpus.append(email_content)` and `labels.append(label)` -- These lines append the file content and corresponding label to their respective lists, `email_corpus` and `labels`.\n",
        "\n",
        "\n",
        "Summary, it reads text files from various directories ('class_files'), stores their contents in `email_corpus`, and records their corresponding labels in `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PXZXmut8mBhc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    email_corpus, labels, test_size=0.2, random_state=11\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOdCtZ1DncJ-"
      },
      "source": [
        "\n",
        "NLP stands for Natural Language Processing. It is a branch of artificial intelligence (AI) and linguistics concerned with the interaction between computers and human (natural) languages. NLP enables computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
        "\n",
        "Key tasks and applications of NLP include:\n",
        "\n",
        "Text Classification and Sentiment Analysis: NLP algorithms can classify text documents into predefined categories or determine the sentiment (positive, negative, or neutral) expressed in text data. This is useful for tasks such as spam detection, sentiment analysis of social media posts, and categorization of news articles.\n",
        "Named Entity Recognition (NER): NLP techniques can identify and extract named entities, such as names of people, organizations, locations, dates, and other entities, from unstructured text data. NER is used in information extraction, document summarization, and entity linking.\n",
        "Part-of-Speech (POS) Tagging: NLP algorithms can assign grammatical tags (e.g., noun, verb, adjective) to each word in a sentence, indicating its syntactic role and grammatical category. POS tagging is important for tasks such as syntactic parsing, machine translation, and speech recognition.\n",
        "Language Modeling and Generation: NLP models can be trained to learn the statistical properties of natural languages and generate coherent and contextually relevant text. Language modeling is used in autocomplete suggestions, machine translation, and dialogue generation applications.\n",
        "Text Summarization: NLP techniques can generate concise summaries of long text documents by extracting key information and main ideas. Text summarization is useful for news aggregation, document summarization, and content generation.\n",
        "Machine Translation: NLP algorithms can translate text from one language to another, enabling cross-lingual communication and information retrieval. Machine translation systems use statistical models, neural networks, or hybrid approaches to achieve accurate translations.\n",
        "Question Answering: NLP systems can analyze natural language questions and provide relevant answers by extracting information from large text corpora or knowledge bases. Question answering systems are used in virtual assistants, search engines, and customer support chatbots.\n",
        "Speech Recognition and Speech Synthesis: NLP techniques are used to transcribe spoken language into text (speech recognition) and generate human-like speech from text input (speech synthesis). Speech recognition and synthesis are used in virtual assistants, dictation systems, and voice-controlled devices.\n",
        "NLP has numerous real-world applications across various industries, including healthcare, finance, e-commerce, education, customer service, and entertainment. It plays a crucial role in enabling human-computer interaction, information retrieval, knowledge discovery, and automation of language-related tasks. As NLP technology continues to advance, it is expected to have a transformative impact on how humans interact with computers and access information in the digital age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "RXPvNUnwnkF5",
        "outputId": "d7d5428a-9b60-4c21-f894-9af250fc4e31"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, HashingVectorizer(ngram_range=(1, 3))),\n",
              "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
              "                (&#x27;dt&#x27;, DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, HashingVectorizer(ngram_range=(1, 3))),\n",
              "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
              "                (&#x27;dt&#x27;, DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HashingVectorizer</label><div class=\"sk-toggleable__content\"><pre>HashingVectorizer(ngram_range=(1, 3))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('vect', HashingVectorizer(ngram_range=(1, 3))),\n",
              "                ('tfidf', TfidfTransformer()),\n",
              "                ('dt', DecisionTreeClassifier(class_weight='balanced'))])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
        "from sklearn import tree\n",
        "\n",
        "nlp_followed_by_dt = Pipeline(\n",
        "    [\n",
        "        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 3))),\n",
        "        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n",
        "        (\"dt\", tree.DecisionTreeClassifier(class_weight=\"balanced\")),\n",
        "    ]\n",
        ")\n",
        "nlp_followed_by_dt.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_oUfhpRpSVo"
      },
      "source": [
        " Firstly, it imports necessary functions: - `Pipeline` from `sklearn.pipeline`. Pipelines streamline the machine learning workflow by chaining consecutive steps of extraction, transformation and assorted processes. - `HashingVectorizer` and `TfidfTransformer` from `sklearn.feature_extraction.text`. These classes are used for text feature extraction. - `tree` from `sklearn`. It's a subpackage that contains tree-based models.\n",
        "\n",
        " 2. Then, it creates the pipeline by calling the `Pipeline` function, which takes a list of (name, transform) tuples, specifying the sequence of steps to be performed. The steps are as follows: - `HashingVectorizer`: A transformer that converts a collection of text documents to a matrix of token occurrences. It's using a hash function to identify tokens and create token counts. - `TfidfTransformer`: It's used to transform a count matrix to a normalized tf (term-frequency) or tf-idf (term frequency-inverse document frequency) representation. Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This transformer applies the tf-idf transformation to the token counts. - `tree.DecisionTreeClassifier`: A classifier that uses a decision tree model. The `class_weight` is set to `balanced` as this will adjust weights inversely proportional to class frequencies in the input data.\n",
        "\n",
        " Finally, `nlp_followed_by_dt.fit(X_train, y_train)` fits the pipeline model to the input training data, `X_train` and `y_train`.\n",
        "\n",
        " So, it's transforming the data using NLP as specified in pipeline and then fitting a decision tree to the transformed data. This pipeline with NLP followed by Decision Tree is a common approach for text classification problems, where we first convert text data into numerical feature vectors, then use a machine learning classifier for classification prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9rgrxKvpsYa",
        "outputId": "3fa0bd06-c817-4cb7-d99e-77e0db7c5f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9948849104859335\n",
            "[[249   3]\n",
            " [  1 529]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "y_test_pred = nlp_followed_by_dt.predict(X_test)\n",
        "print(accuracy_score(y_test, y_test_pred))\n",
        "print(confusion_matrix(y_test, y_test_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
