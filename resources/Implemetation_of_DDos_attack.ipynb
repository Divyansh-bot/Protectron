{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVGdYzN8e4xJ",
        "outputId": "d53ddffa-4d07-405b-d1a6-a577f9a33787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fp5cF6Ge_Ov",
        "outputId": "f65fb1ca-3e13-4970-cc2f-ec475085ac0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t9eXZj7KfZy5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "features = [\n",
        "    \"Fwd Seg Size Min\",\n",
        "    \"Init Bwd Win Byts\",\n",
        "    \"Init Fwd Win Byts\",\n",
        "    \"Fwd Seg Size Min\",\n",
        "    \"Fwd Pkt Len Mean\",\n",
        "    \"Fwd Seg Size Avg\",\n",
        "    \"Label\",\n",
        "    \"Timestamp\",\n",
        "]\n",
        "dtypes = {\n",
        "    \"Fwd Pkt Len Mean\": \"float\",\n",
        "    \"Fwd Seg Size Avg\": \"float\",\n",
        "    \"Init Fwd Win Byts\": \"int\",\n",
        "    \"Init Bwd Win Byts\": \"int\",\n",
        "    \"Fwd Seg Size Min\": \"int\",\n",
        "    \"Label\": \"str\",\n",
        "}\n",
        "date_columns = [\"Timestamp\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tFadyvPgPYn"
      },
      "source": [
        "features` is a list of string items. It contains the names of columns that we're interested in within a dataset.\n",
        "\n",
        "3) `dtypes` is a dictionary mapping column names (as string keys) to their supposed data types (as string values) in the dataset that will be used. For example, the `\"Fwd Pkt Len Mean\"` column data type is a floating point number as indicated by the value `\"float\"`).\n",
        "\n",
        "4) `date_columns` is a list containing the names of the columns where the data stored is of 'date' type. In our case, we only have one column with the name \"Timestamp\" which is of type 'date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMbo6G0vgW0a",
        "outputId": "42d0db96-2fe7-4311-d122-b2a1180e7c46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-f114c526afb2>:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv(\"ddos_dataset.csv\", usecols=features, dtype=dtypes,parse_dates=date_columns,index_col=None)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"ddos_dataset.csv\", usecols=features, dtype=dtypes,parse_dates=date_columns,index_col=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfq2g9j3hMky"
      },
      "source": [
        "The above code is reading a csv file named \"ddos_dataset.csv\" and assigning it to the variable `df`. The `read_csv` function in pandas is able to read in csv files and convert them into DataFrames.- `usecols=features`: This argument is used to select a subset of columns to read from the csv file. The `features` variable must be a list that contains the names of the columns we want to use.- `dtype=dtypes`: This specifies the data types of the columns. The `dtypes` variable should be a dictionary where the keys are the column names and the values are the corresponding datatypes we want to enforce for each column.- `parse_dates=date_columns`: This tells pandas to interpret certain columns as dates. The `date_columns` variable should be a list of column names that contain date information.- `index_col=None`: This specifies which column to use as the row labels in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4maghYqWhXPR"
      },
      "outputs": [],
      "source": [
        "df2 = df.sort_values(\"Timestamp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uNYiObP-hcyL"
      },
      "outputs": [],
      "source": [
        "df3 = df2.drop(columns=[\"Timestamp\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BcCoqcEwhp1V"
      },
      "outputs": [],
      "source": [
        "l = len(df3.index)\n",
        "train_df = df3.head(int(l * 0.8))\n",
        "test_df = df3.tail(int(l * 0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s98Rxne4iZpm"
      },
      "source": [
        "`l = len(df3.index)`: This line is getting the total number of rows in the DataFrame `df3`.\n",
        "\n",
        "2. `train_df = df3.head(int(l * 0.8))`: This line is creating a new DataFrame `train_df`, which consists of the first 80% of the rows from `df3`. The `head` function in pandas returns the first `n` rows for a DataFrame or series. Here `int(l * 0.8)` is calculating 80% of the total number of rows.\n",
        "\n",
        "3. `test_df = df3.tail(int(l * 0.2))`: This line is creating another DataFrame `test_df`, which consists of the last 20% of the rows from `df3`. The `tail` function in pandas returns last `n` rows. Here `int(l * 0.2)` is calculating 20% of the total number of rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iLWXt2tAinAA"
      },
      "outputs": [],
      "source": [
        "y_train = train_df.pop(\"Label\").values\n",
        "y_test = test_df.pop(\"Label\").values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R25XMghVjMRS"
      },
      "source": [
        " `y_train = train_df.pop(\"Label\").values`. This code creates the training label set which contains the true results for the training dataset. It does so by extracting the \"Label\" column from the `train_df` DataFrame, and converting it into a Numpy array.\n",
        "\n",
        " The same with the y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xfWGyosYjLqG"
      },
      "outputs": [],
      "source": [
        "X_train = train_df.values\n",
        "X_test = test_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ToE1vFYyjhtU"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3WZUjAsj265",
        "outputId": "b02fba2e-547e-4e6c-ef27-fb9580e88eec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.83262"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4OAfAOWkzcE"
      },
      "source": [
        "Since the dataset is large, even importing all of it is computationally intensive. For this reason, the first step we begin  by specifying a subset of features from our dataset, the ones we consider most promising, as well as recording their data type so that we don't have to convert them later. We then proceed to read the data into a data frame in the next step. In the next 2 steps, we sort the data by date, since the problem requires being able to predict events in the future, and then drop the date column since we will not be employing it further. In the next two steps, we perform a train-test split, keeping in mind temporal progression. We then instantiate, fit, and test a random forest classifier in the last 2 steps. Depending on the application, the accuracy achieved is a good starting point. A promising direction to improve performance is to account for the source and destination IPs. The reasoning is that, intuitively, where a connection is coming from should have a significant bearing on whether it is part of a DDoS.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
