{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJQe1u_nj2sG",
        "outputId": "9fd7ea2e-8312-4e4d-dd6d-b82178134f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Rx19lvj683",
        "outputId": "db031070-ed94-4d38-c4f4-bf851159d6f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"north_korea_missile_test_database.csv\")\n",
        "y = df[\"Missile Name\"]\n",
        "X = df.drop(\"Missile Name\", axis=1)"
      ],
      "metadata": {
        "id": "ds7kLRISlIXF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explaination:***\n",
        "\n",
        "The above code is splitting a dataset into two parts: features and target. The dataset is about missile tests in North Korea. The data is read from a .csv file using pandas, a popular data manipulation library in Python.\n",
        "\n",
        "Below are the steps:\n",
        "\n",
        "1. `from sklearn.model_selection import train_test_split`: it will import a function train_test_split() from the sklearn library. This function is used to split data into training and testing sets.\n",
        "\n",
        "2. `import pandas as pd`: It will import the pandas library and give it the alias pd. Pandas is widely used for data manipulation and analysis.\n",
        "\n",
        "3. `df = pd.read_csv(\"north_korea_missile_test_database.csv\")`: it will read a CSV (Comma Separated Value) file named \"north_korea_missile_test_database.csv\" and stores the data into a DataFrame. DataFrames are two-dimensional data structures in pandas that can store data of different types (including characters, integers, floating point values, factors and more) in columns.\n",
        "\n",
        "4. `y = df[\"Missile Name\"]`: It will defines the target variable for the machine learning model. The target (y in this case) is what we want our machine learning model to predict. In this case, 'Missile Name' is the target column that contains what type of missile is tested.\n",
        "\n",
        "5. `X = df.drop(\"Missile Name\", axis=1)`: It is for defining the features for the model (X in this case). The features are the input variables that the model uses to make a prediction. drop() is a pandas function that removes the specified column(s) or row(s) from the DataFrame. Because \"Missile Name\" was assigned as the target variable, this line drops the \"Missile Name\" column from the dataset. 'Axis=1' denotes that a column (not a row, which would be axis=0) should be dropped."
      ],
      "metadata": {
        "id": "DgdXHx4JnOYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will need to randomly split the dataset and its labels into a training set consisting 80% of the size of the original dataset and a testing set 20% of the size:"
      ],
      "metadata": {
        "id": "VOKFaraWoMI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=31\n",
        ")"
      ],
      "metadata": {
        "id": "L7YBDr9Nn9cR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X` represents the dataset to be split which contains the independent or input features.\n",
        "- `y`represents the dependent or target variable associated with the dataset `X`.\n",
        "\n",
        "- `test_size = 0.2`: This means 20% of the data will be used for the test split and the rest 80% will be used for training the model.\n",
        "\n",
        "- `random_state = 31`: This is used for initializing the internal random number generator, which decides the splitting of data into train and test indices. Providing an integer as a fixed seed will guarantee that the same sequence of random numbers gets generated each time we run the code. In this case, it would generate the same train/test split each time we run the script.\n",
        "\n",
        "- `X_train, y_train`: These are the features and labels for the training set.\n",
        "\n",
        "- `X_test, y_test`: These are the features and labels for the test set.\n",
        "\n",
        "So the above code is splitting the dataset `X` into training and testing sets, with `X_train` and `X_test` being the respective input features, and `y_train` and `y_test` being the respective labels. It makes 80% of the data as the training set and the remaining 20% as the testing set.\n",
        "\n"
      ],
      "metadata": {
        "id": "y3sUO10-pNi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We apply the train_test_split method once more, to obtain a validation set"
      ],
      "metadata": {
        "id": "q98-fklBp2g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, random_state=31\n",
        ")"
      ],
      "metadata": {
        "id": "mY_8okI1poyp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`X_train, X_val, y_train, y_val`: These variables are placeholders for the training and validation datasets that will be created from the original dataset.\n",
        "\n",
        "- `X_train` and `y_train` denote the features and labels for our training set. - `X_val` and `y_val` denote the features and labels for our validation set.\n",
        "\n",
        "`train_test_split(X_train, y_train, test_size=0.25, random_state=31)``train_test_split`. This is a function from scikit-learn (SKLearn) library used to split a dataset into random train and test subsets.\n",
        "\n",
        "- `X_train, y_train` are inputs to the `train_test_split` function. `X_train` would be the entire set of features, and `y_train` would be the corresponding set of labels.\n",
        "\n",
        "- `test_size=0.25` is an argument that tells the function to reserve 25% of the data for testing (validation in this case). This means that 75% of the data will be used for training the model.- `random_state=31` sets a seed to the random generator, so that our train-test splits are deterministic, meaning that if we call the function again, our data will be split in the same manner. If we do not set a seed, it would generate a new one each time we run it, resulting in different splitting of data each time."
      ],
      "metadata": {
        "id": "0a149rX2qiD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pMCZkLwrJLi",
        "outputId": "9d4e8fb5-4f49-4c24-c6d5-2b25a1c96e20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFzpH-2yrP20",
        "outputId": "066acf8e-4ebc-4b05-fe06-d2527becce2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by reading in our dataset, consisting of historical and continuing missile experiments in North Korea. We aim to predict the type of missile based on remaining features, such as facility and time of launch.\n",
        "\n",
        " we apply scikit-learn's train_test_split method to subdivide X and y into a training set, X_train and y_train, and also a testing set, X_test and y_test. The test_size = 0.2 parameter means that the testing set consists of 20% of the original data, while the remainder is placed in the training set. The random_state parameter allows us to reproduce the same randomly generated split.\n",
        "\n",
        " The danger of using the testing set to select the best model is that we may end up overfitting the testing set. This is similar to the statistical sin of data fishing. In order to combat this danger, we create an additional dataset, called the validation set. We train our models on the training set, use the validation set to compare them, and finally use the testing set to obtain an accurate indicator of the performance of the model we have chosen.\n",
        "\n",
        "we choose our parameters so that, mathematically speaking, the end result consists of a training set of 60% of the original dataset, a validation set of 20%, and a testing set of 20%. Finally, we double-check our assumptions by employing the len function to compute the length of the arrays\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sj4kigwqsI_-"
      }
    }
  ]
}